# Importing libraries
import matplotlib.pyplot as plt
import numpy as np
import operator
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
nltk.download('stopwords')
import sys
import plotly.graph_objs as go
import pandas as pd



# Initializing variables
totalTweets = 0
tweetArray = []
wordArray = []
positiveTotalText = ""
negativeTotalText = ""
positiveSentimentArray = []
neutralSentimentArray = []
negativeSentimentArray = []
splitTweets = []
totalTweetText = ""
dictWordFreq = {}
posWordFreq = {}
negWordFreq = {}
sortedAll = {}
numberOfWords = 5
lati = []
long = []

class Stats:
    def __init__(self, dataArr):
        self.dataarray = dataArr

    def mean(self):
      return float(sum(self.dataarray)/len(self.dataarray))

    def totalcount(self):
     return len(self.dataarray)


    def variance(self):
      mu = self.mean()
      sumv = 0
      for x in self.dataarray:
          sumv = sumv + (x - mu) ** 2
      return float(sumv/self.totalcount())

    def stddev(self):
       stddev = (self.variance()**(.5))
       return stddev

    def median(self):

       self.dataarray.sort()
       if len(self.dataarray) % 2 == 0:
        first_median = self.dataarray[len(self.dataarray) // 2]
        second_median = self.dataarray[len(self.dataarray) // 2 - 1]
        median = (first_median + second_median) / 2
       else:
        median = self.dataarray[len(self.dataarray) // 2]

       return median


class TweetAnalyzer:


  def __init__(self, tweetArr):
      self.tweets = tweetArr

      self.twlenarray = []
      self.wordarray = []

      for tw in self.tweets:
        self.twlenarray.append(len(tw))
        self.wordarray.append(len(tw.split(" ")))

  def wordStats(self):
    return Stats(self.wordarray)

  def tweetStats(self):
    return Stats(self.twlenarray)

  def tweetlenarr(self):
      return self.twlenarray

  def wordarr(self):
      return self.wordarray


class TweetSentimentAnalyzer(TweetAnalyzer):

    def __init__(self,tweetArray):
        super().__init__(tweetArray)
        self.psarray = []
        self.ngarray = []
        self.ntarray = []
        self.dictword = {}
        self.dictpos = {}
        self.dictneg = {}
        self.alltext = ""
        self.postext = ""
        self.negtext = ""
        vader = SentimentIntensityAnalyzer()
        for tw in self.tweets:
            sentimentAnalysis = vader.polarity_scores(tw)
            self.alltext = self.alltext + " " + tw
            if sentimentAnalysis['compound'] > .05:
               self.psarray.append(tw)
               self.postext = self.postext + " " + tw
            elif sentimentAnalysis['compound'] < -.05:
               self.ngarray.append(tw)
               self.negtext = self.negtext + " " + tw
            else:
               self.ntarray.append(tw)


    def psarr(self):
          return self.psarray

    def ngarr(self):
          return self.ngarray

    def ntarr(self):
          return self.ntarray


    def dictWordFreq(self):

        wordFreq(self.dictword, self.alltext)
        return self.dictword

    def posWordFreq(self):

        wordFreq(self.dictpos, self.postext)
        return self.dictpos

    def negWordFreq(self):

        wordFreq(self.dictneg, self.negtext)
        return self.dictneg


def wordFreq(dict, wordText):
    words = wordText.split(" ")
    for word in words:
        if word not in dict:
            dict[word] = 0
        dict[word] += 1

def calculateValues():

    global sentiData

    #Opens csv file using UTF-8 encoding
    csvFile = open('tweeta.csv', 'r', encoding = 'utf-8')
    #Reads csv file
    lines = csvFile.readlines()
    for line in lines:
        splitTweets = line.split("|")
        tmp = splitTweets[2][0:len(splitTweets[2])-1]
        lati.append(splitTweets[1])
        long.append(tmp)
    for tweet in lines:
        # Appends the tweets to an array
        tweetArray.append(tweet.lower())

    sentiData = TweetSentimentAnalyzer(tweetArray)


# Prints all statistics and graphs
def printStats(numberOfWords):
    global sentiData
    # Prints a list of statistics(mean, median, variance) for both characters and words
    print("Average Character Length of Collected Tweets: ", sentiData.tweetStats().mean(), "Total Tweets Collected: ", sentiData.tweetStats().totalcount(), "Standard Deviation of Characters in Collected Tweets: ", sentiData.tweetStats().stddev(),"Median of Characters in Collected Tweets: ", sentiData.tweetStats().median(), "Variance of Characters in Collected Tweets: ", sentiData.tweetStats().variance(), "Average Word Count of Collected Tweets: ", sentiData.wordStats().mean(), "Standard Deviation of Words in Collected Tweets: ", sentiData.wordStats().stddev(), "Variance of Words in Collected Tweets: ", sentiData.wordStats().variance(), "Median of Words in Collected Tweets: ", sentiData.wordStats().median())
    # Prints all positive, neutral, and negative tweets
    print("Positive Tweets: " + str(sentiData.psarr()) + "Neutral/Undecided Tweets:" + str(sentiData.ntarr()) + "Negative Tweets: " + str(sentiData.ngarr()))
    # Prints graphs for characters + words
    printGraphs(numberOfWords)


def printGraphs(numberOfWords):
    global sortedAll
    global lati
    global long
    global sentiData
    # Counts the most common words for all sentiment, positive sentiment, and neutral sentiment
    sortedAll = dict(sorted(sentiData.dictWordFreq().items(), key=operator.itemgetter(1), reverse = True))
    sortedPos = dict(sorted(sentiData.posWordFreq().items(), key=operator.itemgetter(1), reverse = True))
    sortedNeg = dict(sorted(sentiData.negWordFreq().items(), key=operator.itemgetter(1), reverse = True))
    # Bins for graphs
    characterBins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240]
    wordBins = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48]
    # Prints a histogram for characters of collected tweets
    plt.hist(sentiData.tweetlenarr(), bins = characterBins, histtype = 'bar', color='#0504aa', alpha = .7, rwidth = .8)
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel("Number of Characters")
    plt.ylabel("Frequency")
    plt.title("Histogram of Character Frequency")
    plt.text(0, 0, r'$\bar{x}=' + str(round(sentiData.tweetStats().mean(), 2)) + ', Sx= $ ' + str(round(sentiData.tweetStats().stddev(), 2)))
    plt.show()

    # Prints a histogram for words of collected tweets
    plt.hist(sentiData.wordarray, bins = wordBins, histtype = 'bar', color='#0504aa', alpha = .7, rwidth = .8)
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel("Number of Words")
    plt.ylabel("Frequency")
    plt.title("Histogram of Word Frequency")
    plt.text(0, 0, r'$\bar{x}=' + str(round(sentiData.wordStats().mean(), 2)) + ', Sx= $ ' + str(round(sentiData.wordStats().stddev(), 2)))
    plt.show()
    wordGraphs(sortedAll, "", numberOfWords)
    # Does not print a graph if tweets are not found
    if sentiData.psarr() == []:
        print("No positive tweets found")
    else:wordGraphs(sortedPos, "Positive", numberOfWords)
    if sentiData.ngarr() == []:
        print("No negative tweets found")
    else: wordGraphs(sortedNeg, "Negative", numberOfWords)

    #Prints dots on location of tweet on US map
    fig = go.Figure(data= go.Scattergeo(lon = long,lat = lati,mode = 'markers'))
    fig.update_layout(title='Tweet Locations',geo_scope = 'usa')
    fig.show()

    #Creates chloropleth map for COVID-19 cases in the last 7 days based on CDC data
    df = pd.read_csv('casesby7days.csv')
    fig = go.Figure(data=go.Choropleth(locations=df['State/Territory'], z = df['Cases in Last 7 Days'].astype(float), locationmode = 'USA-states',colorscale = 'Reds',colorbar_title = "Thousands of Cases",))
    fig.update_layout(title_text = 'COVID Cases in last 7 Days',geo_scope='usa')
    fig.show()

    # Creates overlay of COVID-19 cases + COVID-19 tweets map
    fig = go.Figure(data=go.Choropleth(locations=df['State/Territory'], z = df['Cases in Last 7 Days'].astype(float), locationmode = 'USA-states',colorscale = 'Reds',colorbar_title = "Thousands of Cases",))
    fig.add_traces(go.Scattergeo(lon=long,lat=lati,mode="markers"))
    fig.update_layout(title_text = 'COVID Cases in last 7 Days',geo_scope='usa')
    fig.show()




def wordGraphs(wordCount, sentiment, numberOfWords):
    #Initializing variables
    info = []
    count = 0
    #Appends keys and values, increases count by 1, if count == numberofwords breaks the program
    for key, value in wordCount.items():
        info.append([key, value])
        count += 1
        if count == numberOfWords:
            break
    #Unpacks values and formats the graph
    names, values = zip(*info)
    ind = np.arange(len(info))
    width = 0.35
    fig, ax = plt.subplots()
    rects1 = ax.bar(ind, values, width, color='r')
    ax.set_ylabel('Frequency')
    ax.set_xlabel('Words')
    ax.set_title('Bar Graph of Most Frequent ' + sentiment + ' Words')
    ax.set_xticks(ind+width/2.)
    ax.set_xticklabels(names)
    #Formatting the graph
    def autolabel(rects):
        for rect in rects:
            height = rect.get_height()
            ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,
                    '%d' % int(height),
                    ha='center', va='bottom')

    autolabel(rects1)
    plt.show()

    # Checks each word in wordCount for words in the remove list, deletes words if found
    for word in stopwords.words():
        if word in wordCount:
            del wordCount[word]
    info = []
    count = 0
    for key, value in wordCount.items():
        info.append([key, value])
        count += 1
        if count == numberOfWords:
            break
    names, values = zip(*info)
    ind = np.arange(len(info))
    width = 0.35
    fig, ax = plt.subplots()
    rects1 = ax.bar(ind, values, width, color='r')
    ax.set_ylabel('Frequency')
    ax.set_xlabel('Words')
    ax.set_title('Bar Graph of Most Frequent ' + sentiment + ' Words(Filtered)')
    ax.set_xticks(ind+width/2.)
    ax.set_xticklabels(names)
    autolabel(rects1)
    plt.show()



# Takes argument for the amount of words displayed on the graph, if not specified it defaults to 5
arguments = len(sys.argv) - 1
if arguments == 1:
    numberOfWords = int(sys.argv[1])

#Runs program
calculateValues()
printStats(numberOfWords)
