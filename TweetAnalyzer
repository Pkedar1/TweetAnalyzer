# Importing libraries
import matplotlib.pyplot as plt
import numpy as np
import operator
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import sys
import plotly.graph_objs as go
import pandas as pd



# Initializing variables
totalTweets = 0
tweetArray = []
wordArray = []
positiveTotalText = ""
negativeTotalText = ""
positiveSentimentArray = []
neutralSentimentArray = []
negativeSentimentArray = []
splitTweets = []
totalTweetText = ""
dictWordFreq = {}
posWordFreq = {}
negWordFreq = {}
sortedAll = {}
numberOfWords = 5
lati = []
long = []

class TweetAnalyzer(tweepy.StreamListener):

def wordFreq(dict, wordText):
    words = wordText.split(" ")
    for word in words:
        if word not in dict:
            dict[word] = 0
        dict[word] += 1



def calculateValues():
    # Creating global variables
    global totalLen
    global totalTweets
    global tweetArray
    global totalWords
    global wordArray
    global positiveSentimentArray
    global neutralSentimentArray
    global negativeSentimentArray
    global totalTweetText
    global positiveTotalText
    global negativeTotalText
    global splitTweets
    global posWordFreq
    global negWordFreq
    global dictWordFreq
    global lati
    global long

    vader = SentimentIntensityAnalyzer()
    #Opens csv file using UTF-8 encoding
    csvFile = open('tweeta.csv', 'r', encoding = 'utf-8')
    #Reads csv file
    lines = csvFile.readlines()
    for line in lines:
        splitTweets = line.split("|")
        tmp = splitTweets[2][0:len(splitTweets[2])-1]
        lati.append(splitTweets[1])
        long.append(tmp)
    for tweet in lines:
        # Counts the number of total tweet
        totalTweets = totalTweets + 1
        # Appends the length of the tweets to an array
        tweetArray.append(len(tweet))
        # Appends the split tweet to itself
        wordArray.append(len(tweet.split(" ")))
        # Adds all text of all tweets
        totalTweetText = totalTweetText + tweet
        # Performs sentinent anaylsis
        sentimentAnalysis = vader.polarity_scores(tweet)
        # Decides whether to classify a tweet as positive/negative/neutral
        if sentimentAnalysis['compound'] > .05:
            positiveSentimentArray.append(tweet)
            positiveTotalText = positiveTotalText + tweet
        elif sentimentAnalysis['compound'] < -.05:
            negativeSentimentArray.append(tweet)
            negativeTotalText = negativeTotalText + tweet
        else:
            neutralSentimentArray.append(tweet)
    #Adds words to dictionaries based on sentiment
    wordFreq(dictWordFreq, totalTweetText)
    wordFreq(posWordFreq, positiveTotalText)
    wordFreq(negWordFreq, negativeTotalText)

# Prints all statistics and graphs
def printStats(numberOfWords):
    # Prints a list of statistics(mean, median, variance) for both characters and words
    print("Average Character Length of Collected Tweets: ", mean(tweetArray), "Total Tweets Collected: ", totalTweets, "Standard Deviation of Characters in Collected Tweets: ", stddev(tweetArray),"Median of Characters in Collected Tweets: ", median(tweetArray), "Variance of Characters in Collected Tweets: ", variance(tweetArray), "Average Word Count of Collected Tweets: ", mean(wordArray), "Standard Deviation of Words in Collected Tweets: ", stddev(wordArray), "Variance of Words in Collected Tweets: ", variance(wordArray), "Median of Words in Collected Tweets: ", median(wordArray))
    # Prints all positive, neutral, and negative tweets
    print("Positive Tweets: " + str(positiveSentimentArray) + "Neutral/Undecided Tweets:" + str(neutralSentimentArray) + "Negative Tweets: " + str(negativeSentimentArray))
    # Prints graphs for characters + words
    printGraphs(numberOfWords)


def printGraphs(numberOfWords):
    global sortedAll
    global lati
    global long
    # Counts the most common words for all sentiment, positive sentiment, and neutral sentiment
    sortedAll = dict(sorted(dictWordFreq.items(), key=operator.itemgetter(1), reverse = True))
    sortedPos = dict(sorted(posWordFreq.items(), key=operator.itemgetter(1), reverse = True))
    sortedNeg = dict(sorted(negWordFreq.items(), key=operator.itemgetter(1), reverse = True))
    # Bins for graphs
    characterBins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240]
    wordBins = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48]
    # Prints a histogram for characters of collected tweets
    plt.hist(tweetArray, bins = characterBins, histtype = 'bar', color='#0504aa', alpha = .7, rwidth = .8)
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel("Number of Characters")
    plt.ylabel("Frequency")
    plt.title("Histogram of Character Frequency")
    plt.text(0, 0, r'$\bar{x}=' + str(round(mean(tweetArray), 2)) + ', Sx= $ ' + str(round(stddev(tweetArray), 2)))
    plt.show()

    # Prints a histogram for words of collected tweets
    plt.hist(wordArray, bins = wordBins, histtype = 'bar', color='#0504aa', alpha = .7, rwidth = .8)
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel("Number of Words")
    plt.ylabel("Frequency")
    plt.title("Histogram of Word Frequency")
    plt.text(0, 0, r'$\bar{x}=' + str(round(mean(wordArray), 2)) + ', Sx= $ ' + str(round(stddev(wordArray), 2)))
    plt.show()
    wordGraphs(sortedAll, "", numberOfWords)
    # Does not print a graph if tweets are not found
    if positiveSentimentArray == []:
        print("No positive tweets found")
    else:wordGraphs(sortedPos, "Positive", numberOfWords)
    if negativeSentimentArray == []:
        print("No negative tweets found")
    else: wordGraphs(sortedNeg, "Negative", numberOfWords)

    #Prints dots on location of tweet on US map
    fig = go.Figure(data= go.Scattergeo(lon = long,lat = lati,mode = 'markers'))
    fig.update_layout(title='Tweet Locations',geo_scope = 'usa')
    fig.show()
    
    #Creates chloropleth map for COVID-19 cases in the last 7 days based on CDC data
    df = pd.read_csv('casesby7days.csv')
    fig = go.Figure(data=go.Choropleth(locations=df['State/Territory'], z = df['Cases in Last 7 Days'].astype(float), locationmode = 'USA-states',colorscale = 'Reds',colorbar_title = "Thousands of Cases",))
    fig.update_layout(title_text = 'COVID Cases in last 7 Days',geo_scope='usa')
    fig.show()

    # Creates overlay of COVID-19 cases + COVID-19 tweets map
    fig = go.Figure(data=go.Choropleth(locations=df['State/Territory'], z = df['Cases in Last 7 Days'].astype(float), locationmode = 'USA-states',colorscale = 'Reds',colorbar_title = "Thousands of Cases",))
    fig.add_traces(go.Scattergeo(lon=long,lat=lati,mode="markers"))
    fig.update_layout(title_text = 'COVID Cases in last 7 Days',geo_scope='usa')
    fig.show()




def wordGraphs(wordCount, sentiment, numberOfWords):
    #Initializing variables
    info = []
    count = 0
    #Appends keys and values, increases count by 1, if count == numberofwords breaks the program
    for key, value in wordCount.items():
        info.append([key, value])
        count += 1
        if count == numberOfWords:
            break
    #Unpacks values and formats the graph
    names, values = zip(*info)
    ind = np.arange(len(info))
    width = 0.35
    fig, ax = plt.subplots()
    rects1 = ax.bar(ind, values, width, color='r')
    ax.set_ylabel('Frequency')
    ax.set_xlabel('Words')
    ax.set_title('Bar Graph of Most Frequent ' + sentiment + ' Words')
    ax.set_xticks(ind+width/2.)
    ax.set_xticklabels(names)
    #Formatting the graph
    def autolabel(rects):
        for rect in rects:
            height = rect.get_height()
            ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,
                    '%d' % int(height),
                    ha='center', va='bottom')

    autolabel(rects1)
    plt.show()

    # Creates filtered bar graph for most common words
    removeList = ["a","an","the","but","by","be","to","of","in","that","have","I","it","for","as","not","this", "and", "on", "is", "The", "would", "are", "were", "u","having", "has", "at", "been", "when", "which", "who", "their", "so", "up", "or", "yet", "=", "with","could","because","into","my", "you", "If", "This", "It", "Since", "all", "All", "since", "was", "Was", "your", "-", "his", "&amp;", "#COVID", "#covid","#COVID-19","#covid-19", "will", "can", " ", "they", "if", "He", "She", "she", "  ", "   ", "from", "us", "we", "A", "our", "You", "Your","how","about", "In", "How", "About", "do","Are","''",'',"", "during", "COVID-19", "COVID", "w/"]
    # Checks each word in wordCount for words in the remove list, deletes words if found
    for word in removeList:
        if word in wordCount:
            del wordCount[word]
    info = []
    count = 0
    for key, value in wordCount.items():
        info.append([key, value])
        count += 1
        if count == numberOfWords:
            break
    names, values = zip(*info)
    ind = np.arange(len(info))
    width = 0.35
    fig, ax = plt.subplots()
    rects1 = ax.bar(ind, values, width, color='r')
    ax.set_ylabel('Frequency')
    ax.set_xlabel('Words')
    ax.set_title('Bar Graph of Most Frequent ' + sentiment + ' Words(Filtered)')
    ax.set_xticks(ind+width/2.)
    ax.set_xticklabels(names)
    autolabel(rects1)
    plt.show()
# Function for mean
def mean(data):
    return float(sum(data) / len(data))
# Function for variance
def variance(data):
    mu = mean(data)
    return mean([(x - mu) ** 2 for x in data])
# Function for standard deviation
def stddev(data):
    stddev = ((variance(data))**(.5))
    return stddev
# Function for median
def median(data):
    data.sort()
    if len(data) % 2 == 0:
        first_median = data[len(data) // 2]
        second_median = data[len(data) // 2 - 1]
        median = (first_median + second_median) / 2
    else:
        median = data[len(data) // 2]
    return median

# Takes argument for the amount of words displayed on the graph, if not specified it defaults to 5
arguments = len(sys.argv) - 1
if arguments == 1:
    numberOfWords = int(sys.argv[1])

#Runs program
calculateValues()
printStats(numberOfWords)
