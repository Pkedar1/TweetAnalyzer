# Importing libraries
import matplotlib.pyplot as plt
import numpy as np
import operator
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
import sys
import plotly.graph_objs as go
import pandas as pd



# Initializing variables
totalTweets = 0
tweetArray = []
wordArray = []
positiveTotalText = ""
negativeTotalText = ""
positiveSentimentArray = []
neutralSentimentArray = []
negativeSentimentArray = []
splitTweets = []
totalTweetText = ""
dictWordFreq = {}
posWordFreq = {}
negWordFreq = {}
sortedAll = {}
numberOfWords = 5
lati = []
long = []
tweetList = []

class Stats:
    def __init__(self, dataArr):
        self.dataarray = dataArr

    def mean(self):
        return float(sum(self.dataarray)/len(self.dataarray))

    def totalcount(self):
        return len(self.dataarray)


    def variance(self):
        mu = self.mean()
        sumv = 0
        for x in self.dataarray:
            sumv = sumv + (x - mu) ** 2
        return float(sumv/self.totalcount())

    def stddev(self):
        stddev = (self.variance()**(.5))
        return stddev

    def median(self):

        self.dataarray.sort()
        if len(self.dataarray) % 2 == 0:
            first_median = self.dataarray[len(self.dataarray) // 2]
            second_median = self.dataarray[len(self.dataarray) // 2 - 1]
            median = (first_median + second_median) / 2
        else:
            median = self.dataarray[len(self.dataarray) // 2]

        return median

class Tweet:

    def __init__(self, tweet, lat, long, sentiment):
        self.tweet = tweet
        self.latitude = lat
        self.longitude = long
        self.sentiment = sentiment

    def tweetlen(self):
        return len(self.tweet)

    def wordlen(self):
        return len(self.tweet.split(" "))

def wordFreq(dict, wordText):
    words = wordText.split(" ")
    for word in words:
        if word not in dict:
            dict[word] = 0
        dict[word] += 1



def calculateValues():
    # Creating global variables
    global totalTweets
    global wordArray
    global positiveSentimentArray
    global neutralSentimentArray
    global negativeSentimentArray
    global totalTweetText
    global positiveTotalText
    global negativeTotalText
    global splitTweets
    global posWordFreq
    global negWordFreq
    global dictWordFreq
    global lati
    global long
    global tweetList

    vader = SentimentIntensityAnalyzer()
    #Opens csv file using UTF-8 encoding
    csvFile = open('car.csv', 'r', encoding = 'utf-8')
    #Reads csv file
    lines = csvFile.readlines()
    for line in lines:
        splitTweets = line.split("|")
        tmp = splitTweets[2][0:len(splitTweets[2])-1]

        atweet = splitTweets[0]
        sentimentAnalysis = vader.polarity_scores(atweet)
        if sentimentAnalysis['compound'] > .05:
            sentiment =  1
        elif sentimentAnalysis['compound'] < -.05:
            sentiment = -1
        else:
            sentiment = 0

        tweetval = Tweet(atweet.lower(),splitTweets[1],tmp,sentiment)
        tweetList.append(tweetval)


    for mytweet in tweetList:
        # Counts the number of total tweet
        totalTweets = totalTweets + 1
        # Appends the length of the tweets to an array
        tweetArray.append(len(mytweet.tweet))
        # Appends the split tweet to itself
        wordArray.append(len(mytweet.tweet.split(" ")))
        # Adds all text of all tweets
        totalTweetText = totalTweetText + mytweet.tweet
        # Performs sentinent anaylsis
        lati.append(mytweet.latitude)
        long.append(mytweet.longitude)
        # Decides whether to classify a tweet as positive/negative/neutral
        if mytweet.sentiment == 1:
            positiveSentimentArray.append(mytweet)
            positiveTotalText = positiveTotalText + mytweet.tweet
        elif mytweet.sentiment == -1:
            negativeSentimentArray.append(mytweet.tweet)
            negativeTotalText = negativeTotalText + mytweet.tweet
        else:
            neutralSentimentArray.append(mytweet.tweet)
    #Adds words to dictionaries based on sentiment
    wordFreq(dictWordFreq, totalTweetText)
    wordFreq(posWordFreq, positiveTotalText)
    wordFreq(negWordFreq, negativeTotalText)

# Prints all statistics and graphs
def printStats(numberOfWords):
    # Prints a list of statistics(mean, median, variance) for both characters and words
    tweetStats = Stats(tweetArray)
    wordStats = Stats(wordArray)
    print("Average Character Length of Collected Tweets: ", tweetStats.mean(), "Total Tweets Collected: ", tweetStats.totalcount(), "Standard Deviation of Characters in Collected Tweets: ", tweetStats.stddev(),"Median of Characters in Collected Tweets: ", tweetStats.median(), "Variance of Characters in Collected Tweets: ", tweetStats.variance(), "Average Word Count of Collected Tweets: ", wordStats.mean(), "Standard Deviation of Words in Collected Tweets: ", wordStats.stddev(), "Variance of Words in Collected Tweets: ", wordStats.variance(), "Median of Words in Collected Tweets: ", wordStats.median())
    # Prints all positive, neutral, and negative tweets
    print("Positive Tweets: " + str(positiveSentimentArray) + "Neutral/Undecided Tweets:" + str(neutralSentimentArray) + "Negative Tweets: " + str(negativeSentimentArray))
    # Prints graphs for characters + words
    printGraphs(numberOfWords)


def printGraphs(numberOfWords):
    tweetStats = Stats(tweetArray)
    wordStats = Stats(wordArray)

    # Counts the most common words for all sentiment, positive sentiment, and neutral sentiment
    sortedAll = dict(sorted(dictWordFreq.items(), key=operator.itemgetter(1), reverse = True))
    sortedPos = dict(sorted(posWordFreq.items(), key=operator.itemgetter(1), reverse = True))
    sortedNeg = dict(sorted(negWordFreq.items(), key=operator.itemgetter(1), reverse = True))
    # Bins for graphs
    characterBins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240]
    wordBins = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48]
    # Prints a histogram for characters of collected tweets
    plt.hist(tweetArray, bins = characterBins, histtype = 'bar', color='#0504aa', alpha = .7, rwidth = .8)
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel("Number of Characters")
    plt.ylabel("Frequency")
    plt.title("Histogram of Character Frequency")
    plt.text(0, 0, r'$\bar{x}=' + str(tweetStats.mean()) + ', Sx= $ ' + str(tweetStats.stddev()))
    plt.show()

    # Prints a histogram for words of collected tweets
    plt.hist(wordArray, bins = wordBins, histtype = 'bar', color='#0504aa', alpha = .7, rwidth = .8)
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel("Number of Words")
    plt.ylabel("Frequency")
    plt.title("Histogram of Word Frequency")
    plt.text(0, 0, r'$\bar{x}=' + str(wordStats.mean()) + ', Sx= $ ' + str(wordStats.stddev()))
    plt.show()
    wordGraphs(sortedAll, "", numberOfWords)
    # Does not print a graph if tweets are not found
    if positiveSentimentArray == []:
        print("No positive tweets found")
    else:wordGraphs(sortedPos, "Positive", numberOfWords)
    if negativeSentimentArray == []:
        print("No negative tweets found")
    else: wordGraphs(sortedNeg, "Negative", numberOfWords)

    #Prints dots on location of tweet on US map
    fig = go.Figure(data= go.Scattergeo(lon = long,lat = lati,mode = 'markers'))
    fig.update_layout(title='Tweet Locations',geo_scope = 'usa')
    fig.show()

    #Creates chloropleth map for COVID-19 cases in the last 7 days based on CDC data
    df = pd.read_csv('casesby7days.csv')
    fig = go.Figure(data=go.Choropleth(locations=df['State/Territory'], z = df['Cases in Last 7 Days'].astype(float), locationmode = 'USA-states',colorscale = 'Reds',colorbar_title = "Thousands of Cases",))
    fig.update_layout(title_text = 'COVID Cases in last 7 Days',geo_scope='usa')
    fig.show()

    # Creates overlay of COVID-19 cases + COVID-19 tweets map
    fig = go.Figure(data=go.Choropleth(locations=df['State/Territory'], z = df['Cases in Last 7 Days'].astype(float), locationmode = 'USA-states',colorscale = 'Reds',colorbar_title = "Thousands of Cases",))
    fig.add_traces(go.Scattergeo(lon=long,lat=lati,mode="markers"))
    fig.update_layout(title_text = 'COVID Cases in last 7 Days',geo_scope='usa')
    fig.show()




def wordGraphs(wordCount, sentiment, numberOfWords):

    tdict = dict(sorted(wordCount.items(), key=operator.itemgetter(1), reverse = True)[:numberOfWords])
    #Unpacks values and formats the graph
    names = tdict.keys()
    values = tdict.values()
    ind = np.arange(len(tdict))
    width = 0.35
    fig, ax = plt.subplots()
    rects1 = ax.bar(ind, values, width, color='r')
    ax.set_ylabel('Frequency')
    ax.set_xlabel('Words')
    ax.set_title('Bar Graph of Most Frequent ' + sentiment + ' Words')
    ax.set_xticks(ind+width/2.)
    ax.set_xticklabels(names)
    #Formatting the graph
    def autolabel(rects):
        for rect in rects:
            height = rect.get_height()
            ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,
                    '%d' % int(height),
                    ha='center', va='bottom')

    autolabel(rects1)
    plt.show()


    # Creates filtered bar graph for most common words
    # Checks each word in wordCount for words in the remove list, deletes words if found
    stopwords = nltk.corpus.stopwords.words('english')
    newStopWords = ['',"-","@", "&amp;", "","\xe2\x80\xa6", "\\xe2\\x80\\xa2"]
    stopwords.extend(newStopWords)
    for word in stopwords:
        if word in wordCount:
            del wordCount[word]


    fdict = dict(sorted(wordCount.items(), key=operator.itemgetter(1), reverse = True)[:numberOfWords])
    names = fdict.keys()
    values = fdict.values()
    ind = np.arange(len(fdict))
    width = 0.35
    fig, ax = plt.subplots()
    rects1 = ax.bar(ind, values, width, color='r')
    ax.set_ylabel('Frequency')
    ax.set_xlabel('Words')
    ax.set_title('Bar Graph of Most Frequent ' + sentiment + ' Words(Filtered)')
    ax.set_xticks(ind+width/2.)
    ax.set_xticklabels(names)
    autolabel(rects1)
    plt.show()

# Takes argument for the amount of words displayed on the graph, if not specified it defaults to 5
arguments = len(sys.argv) - 1
if arguments == 1:
    numberOfWords = int(sys.argv[1])

#Runs program
calculateValues()
printStats(numberOfWords)


