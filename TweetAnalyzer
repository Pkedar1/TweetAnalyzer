################################################################################
# Copyright (c) 2020 Predii, Inc.
#
# Predii Confidential and Proprietary
#
# This software is the confidential and proprietary information of Predii.
# Predii reserves all rights in this software. This software or any portion
# thereof may not be reproduced (in any form whatsoever) except as
# expressly provided by license without the written consent of Predii.################################################################################



# Importing libraries
import matplotlib.pyplot as plt
import numpy as np
import operator
import nltk
from nltk.corpus import stopwords
import sys
import plotly.graph_objs as go
import pandas as pd
from collections import Counter
import plotly as px
from cleantext import clean
from textblob import TextBlob
from unidecode import unidecode
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Initializing variables
totalTweets = 0
tweetArray = []
wordArray = []
positiveTotalText = ""
negativeTotalText = ""
positiveSentimentArray = []
neutralSentimentArray = []
negativeSentimentArray = []
splitTweets = []
totalTweetText = ""
dictWordFreq = {}
posWordFreq = {}
negWordFreq = {}
sortedAll = {}
numberOfWords = 5
lati = []
long = []
tweetList = []
neutralTotalText = ""

class Stats:
    def __init__(self, dataArr):
        self.dataarray = dataArr

    def mean(self):
        return float(sum(self.dataarray)/len(self.dataarray))

    def totalcount(self):
        return len(self.dataarray)


    def variance(self):
        mu = self.mean()
        sumv = 0
        for x in self.dataarray:
            sumv = sumv + (x - mu) ** 2
        return float(sumv/self.totalcount())

    def stddev(self):
        stddev = (self.variance()**(.5))
        return stddev

    def median(self):

        self.dataarray.sort()
        if len(self.dataarray) % 2 == 0:
            first_median = self.dataarray[len(self.dataarray) // 2]
            second_median = self.dataarray[len(self.dataarray) // 2 - 1]
            median = (first_median + second_median) / 2
        else:
            median = self.dataarray[len(self.dataarray) // 2]

        return median

class Tweet:

    def __init__(self, tweet, lat, long, sentiment):
        self.tweet = tweet
        self.latitude = lat
        self.longitude = long
        self.sentiment = sentiment

    def tweetlen(self):
        return len(self.tweet)

    def wordlen(self):
        return len(self.tweet.split(" "))

def wordFreq(dict, wordText):
    words = wordText.split(" ")
    for word in words:
        if word not in dict:
            dict[word] = 0
        dict[word] += 1



def calculateValues():
    # Creating global variables
    global totalTweets
    global wordArray
    global positiveSentimentArray
    global neutralSentimentArray
    global negativeSentimentArray
    global totalTweetText
    global positiveTotalText
    global negativeTotalText
    global splitTweets
    global posWordFreq
    global negWordFreq
    global dictWordFreq
    global lati
    global long
    global tweetList
    global neutralTotalText

    #Opens csv file using UTF-8 encoding
    csvFile = open('2008.txt', 'r',encoding="utf-8")
    #Reads csv file
    lines = csvFile.readlines()
    for line in lines:
        #Finds the text in the file with the review data
        if line.find("<TEXT>", 0, 10) > -1:
            #Cleans the punctuation from the tweet
            atweet = clean(line[6:len(line)-8], no_punct=True)
            #Performs sentiment analysis
            analysis = TextBlob(atweet)
            #Sets the sentiment of the tweet to positive, negative, or neutral
            if analysis.sentiment.polarity > 0:
                sentiment = 1
            elif analysis.sentiment.polarity < 0:
                sentiment = -1
            else:
                sentiment = 0
            tweetval = Tweet(atweet.lower(),0,0,sentiment)
            tweetList.append(tweetval)


    for mytweet in tweetList:
        # Counts the number of total tweet
        totalTweets = totalTweets + 1
        # Appends the length of the tweets to an array
        tweetArray.append(len(mytweet.tweet))
        # Appends the split tweet to itself
        wordArray.append(len(mytweet.tweet.split(" ")))
        # Adds all text of all tweets
        totalTweetText = totalTweetText + mytweet.tweet
        # Adds the longitudes and latitudes of each tweet to a list
        lati.append(mytweet.latitude)
        long.append(mytweet.longitude)
        # Decides whether to classify a tweet as positive/negative/neutral
        if mytweet.sentiment == 1:
            positiveSentimentArray.append(mytweet.tweet)
            positiveTotalText = positiveTotalText + mytweet.tweet
        elif mytweet.sentiment == -1:
            negativeSentimentArray.append(mytweet.tweet)
            negativeTotalText = negativeTotalText + mytweet.tweet
        else:
            neutralSentimentArray.append(mytweet.tweet)
            neutralTotalText = neutralTotalText + mytweet.tweet

    #Adds words to dictionaries based on sentiment
    wordFreq(dictWordFreq, totalTweetText)
    wordFreq(posWordFreq, positiveTotalText)
    wordFreq(negWordFreq, negativeTotalText)

# Prints all statistics and graphs
def printStats(numberOfWords):
    # Prints a list of statistics(mean, median, variance) for both characters and words
    tweetStats = Stats(tweetArray)
    wordStats = Stats(wordArray)
    print("Average Character Length of Collected Tweets: ", tweetStats.mean(), "Total Tweets Collected: ", tweetStats.totalcount(), "Standard Deviation of Characters in Collected Tweets: ", tweetStats.stddev(),"Median of Characters in Collected Tweets: ", tweetStats.median(), "Variance of Characters in Collected Tweets: ", tweetStats.variance(), "Average Word Count of Collected Tweets: ", wordStats.mean(), "Standard Deviation of Words in Collected Tweets: ", wordStats.stddev(), "Variance of Words in Collected Tweets: ", wordStats.variance(), "Median of Words in Collected Tweets: ", wordStats.median())
    # Prints all positive, neutral, and negative tweets
    print("Positive Tweets: "+ str(positiveSentimentArray) )

    print("Neutral/Undecided Tweets:" + str(neutralSentimentArray))
    print("Negative Tweets: " + str(negativeSentimentArray))
    # Prints graphs for characters + words
    printGraphs(numberOfWords)


def printGraphs(numberOfWords):
    tweetStats = Stats(tweetArray)
    wordStats = Stats(wordArray)

    # Counts the most common words for all sentiment, positive sentiment, and neutral sentiment
    sortedAll = dict(sorted(dictWordFreq.items(), key=operator.itemgetter(1), reverse = True))
    sortedPos = dict(sorted(posWordFreq.items(), key=operator.itemgetter(1), reverse = True))
    sortedNeg = dict(sorted(negWordFreq.items(), key=operator.itemgetter(1), reverse = True))
    # Bins for graphs
    characterBins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240]
    wordBins = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48]
    # Prints a histogram for characters of collected tweets
    plt.hist(tweetArray, bins = characterBins, histtype = 'bar', color='#0504aa', alpha = .7, rwidth = .8)
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel("Number of Characters")
    plt.ylabel("Frequency")
    plt.title("Histogram of Character Frequency")
    plt.text(0, 0, r'$\bar{x}=' + str(tweetStats.mean()) + ', Sx= $ ' + str(tweetStats.stddev()))
    plt.show()

    # Prints a histogram for words of collected tweets
    plt.hist(wordArray, bins = wordBins, histtype = 'bar', color='#0504aa', alpha = .7, rwidth = .8)
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel("Number of Words")
    plt.ylabel("Frequency")
    plt.title("Histogram of Word Frequency")
    plt.text(0, 0, r'$\bar{x}=' + str(wordStats.mean()) + ', Sx= $ ' + str(wordStats.stddev()))
    plt.show()
    wordGraphs(sortedAll, "", numberOfWords, neutralTotalText)
    # Does not print a graph if tweets are not found
    if positiveSentimentArray == []:
        print("No positive tweets found")
    else:wordGraphs(sortedPos, "Positive", numberOfWords, positiveTotalText)
    if negativeSentimentArray == []:
        print("No negative tweets found")
    else: wordGraphs(sortedNeg, "Negative", numberOfWords, negativeTotalText)

    #Prints dots on location of tweet on US map
    # fig = go.Figure(data= go.Scattergeo(lon = long,lat = lati,mode = 'markers'))
    #fig.update_layout(title='Tweet Locations',geo_scope = 'usa')
    #fig.show()

    ##Creates chloropleth map for COVID-19 cases in the last 7 days based on CDC data
    #df = pd.read_csv('casesby7days.csv')
    #fig = go.Figure(data=go.Choropleth(locations=df['State/Territory'], z = df['Cases in Last 7 Days'].astype(float), locationmode = 'USA-states',colorscale = 'Reds',colorbar_title = "Thousands of Cases",))
    #fig.update_layout(title_text = 'COVID Cases in last 7 Days',geo_scope='usa')
    #fig.show()

    ## Creates overlay of COVID-19 cases + COVID-19 tweets map
    #fig = go.Figure(data=go.Choropleth(locations=df['State/Territory'], z = df['Cases in Last 7 Days'].astype(float), locationmode = 'USA-states',colorscale = 'Reds',colorbar_title = "Thousands of Cases",))
    #fig.add_traces(go.Scattergeo(lon=long,lat=lati,mode="markers"))
    #fig.update_layout(title_text = 'COVID Cases in last 7 Days',geo_scope='usa')
    #fig.show()

def wordGraphs(wordCount, sentiment, numberOfWords, textType):
    tdict = dict(sorted(wordCount.items(), key=operator.itemgetter(1), reverse = True)[:numberOfWords])
    #Unpacks values and formats the graph
    names = tdict.keys()
    values = tdict.values()
    y_pos = np.arange(len(names))
    plt.barh(y_pos, values, align='center', alpha=0.5)
    plt.yticks(y_pos, names)
    plt.xlabel('Frequency')
    plt.title("Most Frequent " + sentiment + "Words")

    plt.show()

    # Creates filtered bar graph for most common words
    # Checks each word in wordCount for words in the remove list, deletes words if found
    stopwords = nltk.corpus.stopwords.words('english')
    newStopWords = ['',"-","@", "&amp;", "","car", "car.","cars","got","can't", "get", "it.", "would", "much", "ford", "honda", "toyota", "chevrolet", "nissan","isnt"]
    stopwords.extend(newStopWords)
    filteredwords = [word for word in nltk.word_tokenize(textType) if word not in stopwords]

    for word in stopwords:
        if word in wordCount:
            del wordCount[word]

    fdict = dict(sorted(wordCount.items(), key=operator.itemgetter(1), reverse = True)[:numberOfWords])
    names = fdict.keys()
    values = fdict.values()
    y_pos = np.arange(len(names))
    plt.barh(y_pos, values, align='center', alpha=0.5)
    plt.yticks(y_pos, names)
    plt.xlabel('Frequency')
    plt.title("Most Frequent " + sentiment + "Words(Filtered)")

    plt.show()

    countdict = {}
    tuplevaluedict = {}
    for gram in nltk.ngrams(filteredwords, 3):
        for word in names:
            if word in gram:
                wordList = list(gram)
                wordList.sort()
                tupleabc = tuple(wordList)
                myhash = hash(tupleabc)
                if myhash not in countdict:
                    countdict[myhash]=1
                else:
                    countdict[myhash]+=1
                if myhash not in tuplevaluedict.keys():
                    tuplevaluedict[myhash]=gram

    tuples = tuplevaluedict.values()
    counts = countdict.values()
    zip_iterator = zip(tuples,counts)
    tempdict = dict(zip_iterator)
    gdict = dict(sorted(tempdict.items(), key = operator.itemgetter(1), reverse = True))


    names = gdict.keys()
    values = gdict.values()
    y_pos = np.arange(len(names))
    plt.barh(y_pos, values, align='center', alpha=0.5)
    plt.yticks(y_pos, names)
    plt.xlabel('Frequency')
    plt.title("Most Frequent " + sentiment + " Trigrams(Filtered)")

    plt.show()


# Takes argument for the amount of words displayed on the graph, if not specified it defaults to 5
arguments = len(sys.argv) - 1
if arguments == 1:
    numberOfWords = int(sys.argv[1])

#Runs program
calculateValues()
printStats(numberOfWords)


